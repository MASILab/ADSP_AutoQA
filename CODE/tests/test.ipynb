{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json, os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def get_sub_ses_acq_run_t1(t1_path):\n",
    "    pattern = r'(sub-\\w+)(?:_(ses-\\w+))?(?:_(acq-\\w+))?(?:_(run-\\d{1,2}))?_T1w'\n",
    "    matches = re.findall(pattern, t1_path.name)\n",
    "    sub, ses, acq, run = matches[0][0], matches[0][1], matches[0][2], matches[0][3]\n",
    "    return sub, ses, acq, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_cmd = 'find /nfs2/harmonization/BIDS/ABVIB -mindepth 4 -maxdepth 4 \\( -type l -o -type f \\) -name \"*T1w.nii.gz\"'\n",
    "t1s = subprocess.run(find_cmd, shell=True, capture_output=True, text=True).stdout.strip().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:00<00:00, 56435.74it/s]\n"
     ]
    }
   ],
   "source": [
    "#l = t1s[:10]\n",
    "\n",
    "def create_json_dict(filepaths):\n",
    "    \"\"\"\n",
    "    Given a list of filenames, create the initial BIDS json dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    user = os.getlogin()\n",
    "    date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    nested_d = {}\n",
    "    for t1 in tqdm(filepaths):\n",
    "        current_d = nested_d\n",
    "        sub, ses, acq, run = get_sub_ses_acq_run_t1(Path(t1))\n",
    "        for tag in [sub, ses, acq, run]:\n",
    "            if tag:\n",
    "                current_d = current_d.setdefault(tag, {})\n",
    "        #set the default values\n",
    "        row = {'QA_status': 'yes', 'reason': '', 'user': user, 'date': date}\n",
    "        current_d.update(row)\n",
    "        current_d = nested_d\n",
    "\n",
    "    return nested_d\n",
    "\n",
    "    #print(json.dumps(nested_d, indent=4))\n",
    "\n",
    "def convert_json_to_csv(json_dict): ### THERE IS A BUG HERE, NOT SURE WHAT IT IS\n",
    "    \"\"\"\n",
    "    Given a QA JSON dictionary, convert it to a CSV file\n",
    "    \"\"\"\n",
    "\n",
    "    def get_tag_type(d):\n",
    "        tag_types = {\n",
    "            'sub': 'sub',\n",
    "            'ses': 'ses',\n",
    "            'acq': 'acq',\n",
    "            'run': 'run'\n",
    "        }\n",
    "        for key, value in tag_types.items():\n",
    "            if d.startswith(key):\n",
    "                return value\n",
    "        assert False, f\"Unknown tag type: {d}\"\n",
    "\n",
    "    def get_leaf_dicts(d, path=None, curr_dict=None):\n",
    "        if path is None:\n",
    "            path = []\n",
    "        if curr_dict is None:\n",
    "            curr_dict = {}\n",
    "        leaf_dicts = []\n",
    "        for key, value in d.items():\n",
    "            #print(key)\n",
    "            if isinstance(value, dict):\n",
    "                new_path = path + [key]\n",
    "                curr_dict[get_tag_type(key)] = key  #### For some reason, curr_dict is carrying over previous values\n",
    "                leaf_dicts.extend(get_leaf_dicts(value, new_path, curr_dict))\n",
    "            else:\n",
    "                #curr_dict.update(d)\n",
    "                #d.update(curr_dict)\n",
    "                leaf_dicts.append((path, d))\n",
    "                break\n",
    "        return leaf_dicts\n",
    "\n",
    "    #get the leaf dictionaries\n",
    "    leaf_dicts = get_leaf_dicts(json_dict)\n",
    "\n",
    "    #make sure that the paths are included in the leaf dictionaries\n",
    "    for paths,ds in leaf_dicts:\n",
    "        for path in paths:\n",
    "            ds[path[:3]] = path\n",
    "            assert path in ds.values(), f\"Path {path} not in dict {ds}\"\n",
    "        if 'run' not in ds:\n",
    "            ds['run'] = ''\n",
    "        if 'acq' not in ds:\n",
    "            ds['acq'] = ''\n",
    "        if 'ses' not in ds:\n",
    "            ds['ses'] = ''\n",
    "    #now get a list of only the leaf dictionaries\n",
    "    leaf_dicts = [ds for paths,ds in leaf_dicts]\n",
    "    #finally, convert to a csv\n",
    "    header = ['sub', 'ses', 'acq', 'run', 'QA_status', 'reason', 'user', 'date']\n",
    "    df = pd.DataFrame(leaf_dicts)\n",
    "    #reorder the columns accroding to the header\n",
    "    df = df[header]\n",
    "    #replace NaN with empty string\n",
    "    df = df.fillna('')\n",
    "\n",
    "    df.to_csv('QA.csv', index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_csv_to_json(df):\n",
    "    \"\"\"\n",
    "    Given a QA CSV dataframe, convert it to a QA JSON dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    json_data = {}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        #sub, ses, acq, run = row['sub'], row['ses'], row['acq'], row['run']\n",
    "        qa_status, reason, user, date = row['QA_status'], row['reason'], row['user'], row['date']\n",
    "        current_d = json_data\n",
    "        has_d = {}\n",
    "        for tag in ['sub', 'ses', 'acq', 'run']:\n",
    "            if row[tag]:\n",
    "                current_d = current_d.setdefault(row[tag], {})\n",
    "                has_d[tag] = row[tag]\n",
    "        #set the values\n",
    "        add_row = {'QA_status': qa_status, 'reason': reason, 'user': user, 'date': date}\n",
    "        if 'run' not in has_d:\n",
    "            add_row.update({'run': ''})\n",
    "        if 'acq' not in has_d:\n",
    "            add_row.update({'acq': ''})\n",
    "        if 'ses' not in has_d:\n",
    "            add_row.update({'ses': ''})\n",
    "        add_row.update(has_d)\n",
    "        current_d.update(add_row)\n",
    "        current_d = json_data\n",
    "    \n",
    "    #print(json.dumps(json_data, indent=4))\n",
    "\n",
    "    return json_data\n",
    "\n",
    "def compare_dicts(d1, d2):\n",
    "    \"\"\"\n",
    "    Compare two dictionaries\n",
    "    \"\"\"\n",
    "    \n",
    "    #assert len(d1) == len(d2), \"Dictionaries have different lengths\"\n",
    "    for key in d1:\n",
    "        #print(key)\n",
    "        #print(d1)\n",
    "        #print(d2)\n",
    "        assert key in d2, f\"Key {key} not in d2. d1: {d1} \\n d2: {d2}\"\n",
    "        if isinstance(d1[key], dict):\n",
    "            compare_dicts(d1[key], d2[key])\n",
    "        else:\n",
    "            assert d1[key] == d2[key], f\"Values for key {key} are different: {d1[key]} vs {d2[key]}\"\n",
    "\n",
    "\n",
    "nested_d = create_json_dict(t1s)\n",
    "df = convert_json_to_csv(nested_d)\n",
    "converted_json = read_csv_to_json(df)\n",
    "\n",
    "# for x in converted_json:\n",
    "#     print(nested_d[x])\n",
    "#     print(converted_json[x])\n",
    "\n",
    "#print(nested_d['sub-2007'])\n",
    "#print(converted_json['sub-2007'])\n",
    "compare_dicts(nested_d, converted_json)\n",
    "compare_dicts(converted_json, nested_d)\n",
    "\n",
    "#dump the dictionary to a json file\n",
    "# with open('qa1.json', 'w') as f:\n",
    "#     json.dump(nested_d, f, indent=4)\n",
    "\n",
    "# with open('qa2.json', 'w') as f:\n",
    "#     json.dump(converted_json, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs2/harmonization/BIDS/ABVIB/sub-7003/ses-20100408/anat/sub-7003_ses-20100408_acq-SPGR_T1w.nii.gz\n",
      "/nfs2/harmonization/BIDS/ABVIB/sub-201/ses-20090310/anat/sub-201_ses-20090310_acq-MPRAGE_T1w.nii.gz\n",
      "/nfs2/harmonization/BIDS/ABVIB/sub-3831/ses-20120508/anat/sub-3831_ses-20120508_acq-MPRAGE_T1w.nii.gz\n",
      "/nfs2/harmonization/BIDS/ABVIB/sub-61040/ses-20120110/anat/sub-61040_ses-20120110_acq-MPRAGE_T1w.nii.gz\n",
      "/nfs2/harmonization/BIDS/ABVIB/sub-257/ses-20111013/anat/sub-257_ses-20111013_acq-MPRAGE_run-1_T1w.nii.gz\n",
      "/nfs2/harmonization/BIDS/ABVIB/sub-257/ses-20111013/anat/sub-257_ses-20111013_acq-MPRAGE_run-2_T1w.nii.gz\n",
      "/nfs2/harmonization/BIDS/ABVIB/sub-2007/ses-20120320/anat/sub-2007_ses-20120320_acq-SPGR_T1w.nii.gz\n",
      "/nfs2/harmonization/BIDS/ABVIB/sub-7055/ses-20120712/anat/sub-7055_ses-20120712_acq-SPGR_T1w.nii.gz\n",
      "/nfs2/harmonization/BIDS/ABVIB/sub-60024/ses-20100909/anat/sub-60024_ses-20100909_acq-MPRAGE_T1w.nii.gz\n",
      "/nfs2/harmonization/BIDS/ABVIB/sub-218/ses-20121108/anat/sub-218_ses-20121108_acq-MPRAGE_T1w.nii.gz\n"
     ]
    }
   ],
   "source": [
    "for x in t1s[:10]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:00<00:00, 64980.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "PNG sub-2007_ses-2019_WMAtlasacq-MPRrun-1.png has no corresponding json entry. Adding to json file.\n",
      "PNG sub-2007_ses-2019_WMAtlasacq-MPRrun-1.png has no corresponding json entry. Adding to json file.\n",
      "PNG sub-2007_ses-2019_WMAtlasacq-MPRrun-1.png has no corresponding json entry. Adding to json file.\n",
      "{'QA_status': 'yes', 'reason': '', 'user': 'root', 'date': '2024-07-09 22:27:22', 'sub': 'sub-2007', 'ses': 'ses-2019', 'acq': 'acq-MPR', 'run': 'run-1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## test read in the json\n",
    "import json\n",
    "\n",
    "def get_tag_type(d):\n",
    "    tag_types = {\n",
    "        'sub': 'sub',\n",
    "        'ses': 'ses',\n",
    "        'acq': 'acq',\n",
    "        'run': 'run'\n",
    "    }\n",
    "    for key, value in tag_types.items():\n",
    "        if d.startswith(key):\n",
    "            return value\n",
    "    assert False, f\"Unknown tag type: {d}\"\n",
    "\n",
    "def get_leaf_dicts(d, path=None, curr_dict=None):\n",
    "    if path is None:\n",
    "        path = []\n",
    "    if curr_dict is None:\n",
    "        curr_dict = {}\n",
    "    leaf_dicts = []\n",
    "    for key, value in d.items():\n",
    "        #print(key)\n",
    "        if isinstance(value, dict):\n",
    "            new_path = path + [key]\n",
    "            curr_dict[get_tag_type(key)] = key  #### For some reason, curr_dict is carrying over previous values\n",
    "            leaf_dicts.extend(get_leaf_dicts(value, new_path, curr_dict))\n",
    "        else:\n",
    "            leaf_dicts.append((path, d))\n",
    "            break\n",
    "    return leaf_dicts\n",
    "\n",
    "def are_unique_qa_dicts(dict_list):\n",
    "    \"\"\"\n",
    "    Given a list of qa dictionaries, check that no two dictionaries are the same\n",
    "\n",
    "    Only considers the sub, ses, acq, and run elements\n",
    "    \"\"\"\n",
    "\n",
    "    def add_items(curr_set, elt):\n",
    "        curr_set.add(elt)\n",
    "        return len(curr_set)\n",
    "\n",
    "    seen = set()\n",
    "    for d in tqdm(dict_list):\n",
    "        sub, ses, acq, run = d['sub'], d['ses'], d['acq'], d['run']\n",
    "        if len(seen) == add_items(seen, (sub, ses, acq, run)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def _create_pngs(t1s):\n",
    "    pngs = []\n",
    "    for t1 in t1s:\n",
    "        sub, ses, acq, run = get_sub_ses_acq_run_t1(Path(t1))\n",
    "        png = f'{sub}_'\n",
    "        if ses:\n",
    "            png += f\"{ses}_\"\n",
    "        png += \"WMAtlas\"\n",
    "        if acq:\n",
    "            png += f\"{acq}\"\n",
    "        if run:\n",
    "            png += f\"{run}\"\n",
    "        png += \".png\"\n",
    "        pngs.append(png)\n",
    "\n",
    "    return pngs\n",
    "\n",
    "def assert_tags_in_dict(paths, leaf_dicts):\n",
    "    \"\"\"\n",
    "    For given lists of paths and leaf dictionaries, assert that the paths are in the dictionaries\n",
    "    \"\"\"\n",
    "    for paths,ds in zip(paths, leaf_dicts):\n",
    "        for path in paths:\n",
    "            assert path in ds.values(), f\"Path {path} not in dict {ds}\"\n",
    "\n",
    "def get_BIDS_fields_from_png(filename, return_pipeline=False):\n",
    "    \"\"\"\n",
    "    Given a QA png filename, return the BIDS fields.\n",
    "    \"\"\"\n",
    "    #pattern = r\"sub-(?P<sub>\\d+)_ses-(?P<ses>\\d+)_\\w+acq-(?P<acq>\\d+)run-(?P<run>\\d+)\\.png\"\n",
    "    pattern = r'(sub-\\w+)(?:_(ses-\\w+))?(?:_(\\w+))(?:(acq-\\w+))?(?:(run-\\d{1,2}))?.png'\n",
    "    match = re.match(pattern, filename)\n",
    "    assert match, f\"Filename {filename} does not match the expected pattern.\"\n",
    "    tags = {'sub': match.group(1), 'ses': match.group(2), 'acq': match.group(4), 'run': match.group(5)}\n",
    "    if return_pipeline:\n",
    "        tags['pipeline'] = match.group(3)\n",
    "    return tags\n",
    "\n",
    "\n",
    "def check_png_for_json(dicts, pngs):\n",
    "    \"\"\"\n",
    "    Given a list of QA json leaf dictionaries and list of pngs, make sure that every single json entry has a corresponding png file\n",
    "    \"\"\"\n",
    "\n",
    "    #get the pipeline\n",
    "    pipeline = get_BIDS_fields_from_png(pngs[0], return_pipeline=True)['pipeline']\n",
    "\n",
    "    for dic in dicts:\n",
    "        sub, ses, acq, run = dic['sub'], dic['ses'], dic['acq'], dic['run']\n",
    "        png = f'{sub}_'\n",
    "        if ses:\n",
    "            png += f\"{ses}_\"\n",
    "        png += f\"{pipeline}\"\n",
    "        if acq:\n",
    "            png += f\"{acq}\"\n",
    "        if run:\n",
    "            png += f\"{run}\"\n",
    "        png += \".png\"\n",
    "        assert png in pngs, f\"PNG {png} from {dic} not in list of pngs\"\n",
    "\n",
    "def check_json_for_png(nested, pngs):\n",
    "    \"\"\"\n",
    "    Given a nested json and list of pngs, make sure that every single png file has a corresponding json entry.\n",
    "\n",
    "    If it does not, then add the default values to the json file.\n",
    "    \"\"\"\n",
    "\n",
    "    user = os.getlogin()\n",
    "    date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    for png in pngs:\n",
    "        tags = get_BIDS_fields_from_png(png)\n",
    "        sub, ses, acq, run = tags['sub'], tags['ses'], tags['acq'], tags['run']\n",
    "        current_d = nested\n",
    "        for tag in [sub, ses, acq, run]:\n",
    "            if tag:\n",
    "                try:\n",
    "                    current_d = current_d[tag]\n",
    "                except KeyError:\n",
    "                    print(f\"PNG {png} has no corresponding json entry. Adding to json file.\")\n",
    "                    current_d = current_d.setdefault(tag, {})\n",
    "        #if current_d is blank, then we need to add the default values\n",
    "        if not current_d:\n",
    "            row = {'QA_status': 'yes', 'reason': '', 'user': os.getlogin(), 'date': date}\n",
    "            current_d.update(row)\n",
    "            current_d.update(tags)\n",
    "    \n",
    "    return nested\n",
    "                    \n",
    "\n",
    "jsonf = \"qa1.json\"\n",
    "with open(jsonf, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "#check to make sure that every single json entry has a corresponding png file\n",
    "t1s_f = \"t1s.txt\"\n",
    "with open(t1s_f, 'r') as f:\n",
    "    t1s = f.readlines()\n",
    "    t1s = [x.strip() for x in t1s]\n",
    "\n",
    "paths, leaf_dicts = zip(*get_leaf_dicts(data))\n",
    "print(are_unique_qa_dicts(leaf_dicts))\n",
    "\n",
    "#now, go through every single path and ds combination to make sure that the path is represented in the ds\n",
    "assert_tags_in_dict(paths, leaf_dicts)\n",
    "\n",
    "#now make sure that every single json entry has a corresponding png file\n",
    "pngs = _create_pngs(t1s)\n",
    "check_png_for_json(leaf_dicts, pngs)\n",
    "\n",
    "#now, make sure that every single png file has a corresponding json entry. If it does not, then we need to add to the json file\n",
    "pngs += ['sub-2007_ses-2019_WMAtlasacq-MPRrun-1.png']\n",
    "data = check_json_for_png(data, pngs)\n",
    "print(data['sub-2007']['ses-2019']['acq-MPR']['run-1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({2, 4})\n",
      "{frozenset({2, 4})}\n"
     ]
    }
   ],
   "source": [
    "#check that no two dictionaries in a list are the same\n",
    "def are_unique_dicts(dict_list):\n",
    "    \"\"\"\n",
    "    Given a list of dictionaries, check that no two dictionaries are the same\n",
    "\n",
    "    First, only considers the sub, ses, acq, and run elements\n",
    "    \"\"\"\n",
    "\n",
    "    def add_items(curr_set, elt):\n",
    "        curr_set.add(elt)\n",
    "        return len(curr_set)\n",
    "\n",
    "    seen = set()\n",
    "    for d in dict_list:\n",
    "        sub, ses, acq, run = d['sub'], d['ses'], d['acq'], d['run']\n",
    "        if len(seen) == add_items(seen, (sub, ses, acq, run)):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sub-7003_ses-20100408_WMAtlasacq-SPGR.png', 'sub-201_ses-20090310_WMAtlasacq-MPRAGE.png', 'sub-3831_ses-20120508_WMAtlasacq-MPRAGE.png', 'sub-61040_ses-20120110_WMAtlasacq-MPRAGE.png', 'sub-257_ses-20111013_WMAtlasacq-MPRAGErun-1.png', 'sub-257_ses-20111013_WMAtlasacq-MPRAGErun-2.png', 'sub-2007_ses-20120320_WMAtlasacq-SPGR.png', 'sub-7055_ses-20120712_WMAtlasacq-SPGR.png', 'sub-60024_ses-20100909_WMAtlasacq-MPRAGE.png', 'sub-218_ses-20121108_WMAtlasacq-MPRAGE.png', 'sub-262_ses-20111123_WMAtlasacq-MPRAGE.png', 'sub-268_ses-20111129_WMAtlasacq-MPRAGE.png', 'sub-286_ses-20120531_WMAtlasacq-MPRAGE.png', 'sub-159_ses-20090519_WMAtlasacq-MPRAGE.png', 'sub-159_ses-20110520_WMAtlasacq-MPRAGE.png', 'sub-60008_ses-20120321_WMAtlasacq-MPRAGE.png', 'sub-7004_ses-20120322_WMAtlasacq-SPGR.png', 'sub-7004_ses-20100322_WMAtlasacq-SPGR.png', 'sub-1031_ses-20110623_WMAtlasacq-SPGR.png', 'sub-3592_ses-20110104_WMAtlasacq-MPRAGE.png', 'sub-3471_ses-20100526_WMAtlasacq-MPRAGE.png', 'sub-7045_ses-20120106_WMAtlasacq-SPGR.png', 'sub-61034_ses-20120124_WMAtlasacq-MPRAGE.png', 'sub-157_ses-20090616_WMAtlasacq-MPRAGE.png', 'sub-1067_ses-20110520_WMAtlasacq-SPGR.png', 'sub-3654_ses-20110505_WMAtlasacq-MPRAGE.png', 'sub-3623_ses-20130211_WMAtlasacq-MPRAGE.png', 'sub-1060_ses-20110627_WMAtlasacq-SPGR.png', 'sub-1183_ses-20111026_WMAtlasacq-SPGR.png', 'sub-3380_ses-20101005_WMAtlasacq-MPRAGE.png', 'sub-3380_ses-20130419_WMAtlasacq-MPRAGE.png', 'sub-58081_ses-20120919_WMAtlasacq-MPRAGE.png', 'sub-58081_ses-20101112_WMAtlasacq-MPRAGE.png', 'sub-3586_ses-20130122_WMAtlasacq-MPRAGE.png', 'sub-3685_ses-20110712_WMAtlasacq-MPRAGE.png', 'sub-7014_ses-20101124_WMAtlasacq-SPGR.png', 'sub-59057_ses-20100924_WMAtlasacq-MPRAGE.png', 'sub-171_ses-20091217_WMAtlasacq-MPRAGE.png', 'sub-3640_ses-20110413_WMAtlasacq-MPRAGE.png', 'sub-7037_ses-20110818_WMAtlasacq-SPGR.png', 'sub-235_ses-20110516_WMAtlasacq-MPRAGE.png', 'sub-1034_ses-20110620_WMAtlasacq-SPGR.png', 'sub-1139_ses-20090528_WMAtlasacq-SPGR.png', 'sub-136_ses-20100113_WMAtlasacq-MPRAGE.png', 'sub-263_ses-20111115_WMAtlasacq-MPRAGE.png', 'sub-1112_ses-20110207_WMAtlasacq-SPGR.png', 'sub-221_ses-20101104_WMAtlasacq-MPRAGE.png', 'sub-7011_ses-20101007_WMAtlasacq-SPGR.png', 'sub-7029_ses-20130514_WMAtlasacq-SPGR.png', 'sub-264_ses-20111201_WMAtlasacq-MPRAGE.png', 'sub-146_ses-20090903_WMAtlasacq-MPRAGE.png', 'sub-146_ses-20111121_WMAtlasacq-MPRAGE.png', 'sub-7030_ses-20110303_WMAtlasacq-SPGR.png', 'sub-3868_ses-20130227_WMAtlasacq-MPRAGE.png', 'sub-1191_ses-20110621_WMAtlasacq-SPGR.png', 'sub-204_ses-20091217_WMAtlasacq-MPRAGE.png', 'sub-3477_ses-20130108_WMAtlasacq-MPRAGE.png', 'sub-3790_ses-20120507_WMAtlasacq-MPRAGE.png', 'sub-1147_ses-20091013_WMAtlasacq-SPGR.png', 'sub-7006_ses-20120913_WMAtlasacq-SPGR.png', 'sub-163_ses-20091015_WMAtlasacq-MPRAGE.png', 'sub-260_ses-20111130_WMAtlasacq-MPRAGE.png', 'sub-7050_ses-20120626_WMAtlasacq-SPGR.png', 'sub-7027_ses-20130228_WMAtlasacq-SPGR.png', 'sub-7027_ses-20110217_WMAtlasacq-SPGR.png', 'sub-61032_ses-20111117_WMAtlasacq-MPRAGE.png', 'sub-2918_ses-20130214_WMAtlasacq-MPRAGE.png', 'sub-2005_ses-20090526_WMAtlasacq-SPGR.png', 'sub-7001_ses-20090729_WMAtlasacq-SPGR.png', 'sub-7001_ses-20130826_WMAtlasacq-SPGR.png', 'sub-145_ses-20090407_WMAtlasacq-MPRAGE.png', 'sub-1075_ses-20100701_WMAtlasacq-SPGR.png', 'sub-209_ses-20100729_WMAtlasacq-MPRAGE.png', 'sub-2469_ses-20090622_WMAtlasacq-MPRAGE.png', 'sub-2469_ses-20110315_WMAtlasacq-MPRAGE.png']\n"
     ]
    }
   ],
   "source": [
    "#'sub-1_ses-1_WMAtlasacq-1run-1.png'\n",
    "def _create_pngs(t1s):\n",
    "    pngs = []\n",
    "    for t1 in t1s:\n",
    "        sub, ses, acq, run = get_sub_ses_acq_run_t1(Path(t1))\n",
    "        png = f'{sub}_'\n",
    "        if ses:\n",
    "            png += f\"{ses}_\"\n",
    "        png += \"WMAtlas\"\n",
    "        if acq:\n",
    "            png += f\"{acq}\"\n",
    "        if run:\n",
    "            png += f\"{run}\"\n",
    "        png += \".png\"\n",
    "        pngs.append(png)\n",
    "\n",
    "    return pngs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
