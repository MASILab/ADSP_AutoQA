{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json, os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def get_sub_ses_acq_run_t1(t1_path):\n",
    "    pattern = r'(sub-\\w+)(?:_(ses-\\w+))?(?:_(acq-\\w+))?(?:_(run-\\d{1,2}))?_T1w'\n",
    "    matches = re.findall(pattern, t1_path.name)\n",
    "    sub, ses, acq, run = matches[0][0], matches[0][1], matches[0][2], matches[0][3]\n",
    "    return sub, ses, acq, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_cmd = 'find /nfs2/harmonization/BIDS/ABVIB -mindepth 4 -maxdepth 4 \\( -type l -o -type f \\) -name \"*T1w.nii.gz\"'\n",
    "t1s = subprocess.run(find_cmd, shell=True, capture_output=True, text=True).stdout.strip().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:00<00:00, 56435.74it/s]\n"
     ]
    }
   ],
   "source": [
    "#l = t1s[:10]\n",
    "\n",
    "def create_json_dict(filepaths):\n",
    "    \"\"\"\n",
    "    Given a list of filenames, create the initial BIDS json dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    user = os.getlogin()\n",
    "    date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    nested_d = {}\n",
    "    for t1 in tqdm(filepaths):\n",
    "        current_d = nested_d\n",
    "        sub, ses, acq, run = get_sub_ses_acq_run_t1(Path(t1))\n",
    "        for tag in [sub, ses, acq, run]:\n",
    "            if tag:\n",
    "                current_d = current_d.setdefault(tag, {})\n",
    "        #set the default values\n",
    "        row = {'QA_status': 'yes', 'reason': '', 'user': user, 'date': date}\n",
    "        current_d.update(row)\n",
    "        current_d = nested_d\n",
    "\n",
    "    return nested_d\n",
    "\n",
    "    #print(json.dumps(nested_d, indent=4))\n",
    "\n",
    "def convert_json_to_csv(json_dict): ### THERE IS A BUG HERE, NOT SURE WHAT IT IS\n",
    "    \"\"\"\n",
    "    Given a QA JSON dictionary, convert it to a CSV file\n",
    "    \"\"\"\n",
    "\n",
    "    def get_tag_type(d):\n",
    "        tag_types = {\n",
    "            'sub': 'sub',\n",
    "            'ses': 'ses',\n",
    "            'acq': 'acq',\n",
    "            'run': 'run'\n",
    "        }\n",
    "        for key, value in tag_types.items():\n",
    "            if d.startswith(key):\n",
    "                return value\n",
    "        assert False, f\"Unknown tag type: {d}\"\n",
    "\n",
    "    def get_leaf_dicts(d, path=None, curr_dict=None):\n",
    "        if path is None:\n",
    "            path = []\n",
    "        if curr_dict is None:\n",
    "            curr_dict = {}\n",
    "        leaf_dicts = []\n",
    "        for key, value in d.items():\n",
    "            #print(key)\n",
    "            if isinstance(value, dict):\n",
    "                new_path = path + [key]\n",
    "                curr_dict[get_tag_type(key)] = key  #### For some reason, curr_dict is carrying over previous values\n",
    "                leaf_dicts.extend(get_leaf_dicts(value, new_path, curr_dict))\n",
    "            else:\n",
    "                #curr_dict.update(d)\n",
    "                #d.update(curr_dict)\n",
    "                leaf_dicts.append((path, d))\n",
    "                break\n",
    "        return leaf_dicts\n",
    "\n",
    "    #get the leaf dictionaries\n",
    "    leaf_dicts = get_leaf_dicts(json_dict)\n",
    "\n",
    "    #make sure that the paths are unique\n",
    "    for paths,ds in leaf_dicts:\n",
    "        for path in paths:\n",
    "            ds[path[:3]] = path\n",
    "            assert path in ds.values(), f\"Path {path} not in dict {ds}\"\n",
    "        if 'run' not in ds:\n",
    "            ds['run'] = ''\n",
    "        if 'acq' not in ds:\n",
    "            ds['acq'] = ''\n",
    "        if 'ses' not in ds:\n",
    "            ds['ses'] = ''\n",
    "    #now get a list of only the leaf dictionaries\n",
    "    leaf_dicts = [ds for paths,ds in leaf_dicts]\n",
    "    #finally, convert to a csv\n",
    "    header = ['sub', 'ses', 'acq', 'run', 'QA_status', 'reason', 'user', 'date']\n",
    "    df = pd.DataFrame(leaf_dicts)\n",
    "    #reorder the columns accroding to the header\n",
    "    df = df[header]\n",
    "    #replace NaN with empty string\n",
    "    df = df.fillna('')\n",
    "\n",
    "    df.to_csv('qa.csv', index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_csv_to_json(df):\n",
    "    \"\"\"\n",
    "    Given a QA CSV dataframe, convert it to a QA JSON dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    json_data = {}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        #sub, ses, acq, run = row['sub'], row['ses'], row['acq'], row['run']\n",
    "        qa_status, reason, user, date = row['QA_status'], row['reason'], row['user'], row['date']\n",
    "        current_d = json_data\n",
    "        has_d = {}\n",
    "        for tag in ['sub', 'ses', 'acq', 'run']:\n",
    "            if row[tag]:\n",
    "                current_d = current_d.setdefault(row[tag], {})\n",
    "                has_d[tag] = row[tag]\n",
    "        #set the values\n",
    "        add_row = {'QA_status': qa_status, 'reason': reason, 'user': user, 'date': date}\n",
    "        if 'run' not in has_d:\n",
    "            add_row.update({'run': ''})\n",
    "        if 'acq' not in has_d:\n",
    "            add_row.update({'acq': ''})\n",
    "        if 'ses' not in has_d:\n",
    "            add_row.update({'ses': ''})\n",
    "        add_row.update(has_d)\n",
    "        current_d.update(add_row)\n",
    "        current_d = json_data\n",
    "    \n",
    "    #print(json.dumps(json_data, indent=4))\n",
    "\n",
    "    return json_data\n",
    "\n",
    "def compare_dicts(d1, d2):\n",
    "    \"\"\"\n",
    "    Compare two dictionaries\n",
    "    \"\"\"\n",
    "    \n",
    "    #assert len(d1) == len(d2), \"Dictionaries have different lengths\"\n",
    "    for key in d1:\n",
    "        #print(key)\n",
    "        #print(d1)\n",
    "        #print(d2)\n",
    "        assert key in d2, f\"Key {key} not in d2. d1: {d1} \\n d2: {d2}\"\n",
    "        if isinstance(d1[key], dict):\n",
    "            compare_dicts(d1[key], d2[key])\n",
    "        else:\n",
    "            assert d1[key] == d2[key], f\"Values for key {key} are different: {d1[key]} vs {d2[key]}\"\n",
    "\n",
    "\n",
    "nested_d = create_json_dict(t1s)\n",
    "df = convert_json_to_csv(nested_d)\n",
    "converted_json = read_csv_to_json(df)\n",
    "\n",
    "# for x in converted_json:\n",
    "#     print(nested_d[x])\n",
    "#     print(converted_json[x])\n",
    "\n",
    "#print(nested_d['sub-2007'])\n",
    "#print(converted_json['sub-2007'])\n",
    "compare_dicts(nested_d, converted_json)\n",
    "compare_dicts(converted_json, nested_d)\n",
    "\n",
    "#dump the dictionary to a json file\n",
    "# with open('qa1.json', 'w') as f:\n",
    "#     json.dump(nested_d, f, indent=4)\n",
    "\n",
    "# with open('qa2.json', 'w') as f:\n",
    "#     json.dump(converted_json, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs2/harmonization/BIDS/ABVIB/sub-7003/ses-20100408/anat/sub-7003_ses-20100408_acq-SPGR_T1w.nii.gz\n",
      "/nfs2/harmonization/BIDS/ABVIB/sub-201/ses-20090310/anat/sub-201_ses-20090310_acq-MPRAGE_T1w.nii.gz\n",
      "/nfs2/harmonization/BIDS/ABVIB/sub-3831/ses-20120508/anat/sub-3831_ses-20120508_acq-MPRAGE_T1w.nii.gz\n",
      "/nfs2/harmonization/BIDS/ABVIB/sub-61040/ses-20120110/anat/sub-61040_ses-20120110_acq-MPRAGE_T1w.nii.gz\n",
      "/nfs2/harmonization/BIDS/ABVIB/sub-257/ses-20111013/anat/sub-257_ses-20111013_acq-MPRAGE_run-1_T1w.nii.gz\n",
      "/nfs2/harmonization/BIDS/ABVIB/sub-257/ses-20111013/anat/sub-257_ses-20111013_acq-MPRAGE_run-2_T1w.nii.gz\n",
      "/nfs2/harmonization/BIDS/ABVIB/sub-2007/ses-20120320/anat/sub-2007_ses-20120320_acq-SPGR_T1w.nii.gz\n",
      "/nfs2/harmonization/BIDS/ABVIB/sub-7055/ses-20120712/anat/sub-7055_ses-20120712_acq-SPGR_T1w.nii.gz\n",
      "/nfs2/harmonization/BIDS/ABVIB/sub-60024/ses-20100909/anat/sub-60024_ses-20100909_acq-MPRAGE_T1w.nii.gz\n",
      "/nfs2/harmonization/BIDS/ABVIB/sub-218/ses-20121108/anat/sub-218_ses-20121108_acq-MPRAGE_T1w.nii.gz\n"
     ]
    }
   ],
   "source": [
    "for x in t1s[:10]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
